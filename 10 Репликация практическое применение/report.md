# Асинхронная репликация

## Как выполнялось тестирование
В JMeter в 50 потоках в течение 1 минуты вызывалась АПИ /user/search и в неё передавались параметры, полученные из того же csv файла, который использовался для заполнения таблицы тестовыми данными. Это нужно, чтобы избежать кэширования данных на стороне постгриса.

Одновременно в 50 потоках в течение 1 минуты вызывалась АПИ /user/register, в которую передавались случайные данные для создания пользователей (берутся из другого генерёного csv файла)

Была настроена асинхронная репликация.

В первой части эксперимента обе АПИ, и /user/search, и /user/register смотрели только на мастер.

Во второй части эксперимента /user/search смотрела на слейв, а /user/register - на мастер.

До и между эксперементами удалялись вставленные записи, чтобы тестирование производилось на одинаковом наборе данных.
Перезапускались инстансы мастера и слейва, чтобы обнулить кэш страниц.
Выполнялся сброс статистики pg_stat_statements:
`SELECT pg_stat_statements_reset();`

Микросервис запускался в докере с параметром --cpus=10
Мастер и слейв запускались в докере с параметрами --cpus=1 -m 1Gb

Параметры Latency, Throughput брались из JMeter.
CPU и Mem из docker stats

Данные о вводе-выводе брались из  из pg_stat_statements:

    SELECT sum(shared_blks_read) as "количество страниц прочитано с диска"
    ,sum(shared_blks_written) as "количество страниц записано на диск"
    FROM pg_stat_statements;`

## Результаты тестирования, когда обе АПИ смотрели на мастер
| API             | Latency 95%, ms | Throughput, rps | CPU master, % | Mem maser, mb | CPU slave, % | Mem slave, mb | Страниц прочитано master | Страниц записано master | Страниц прочитано slave | Страниц записано slave |   
|-----------------|-----------------|-----------------|---------------|---------------|--------------|---------------|--------------------------|-------------------------|-------------------------|------------------------|
| /user/search    | 83              | 4073.1          | 176           | 101           | 0.38         | 82            | 22136                    | 33                      | 2                       | 0                      |
| /user/register  | 2518            | 40.7            | 176           | 101           | 0.38         | 82            | 22136                    | 33                      | 2                       | 0                      |


## Результаты тестирования, когда /user/search сотрела на slave
| API             | Latency 95%, ms | Throughput, rps | CPU master, % | Mem maser, mb | CPU slave, % | Mem slave, mb | Страниц прочитано master | Страниц записано master | Страниц прочитано slave | Страниц записано slave |
|-----------------|-----------------|-----------------|---------------|---------------|--------------|---------------|--------------------------|-------------------------|-------------------------|------------------------|
| /user/search    | 80              | 4512            | 2.4           | 165           | 101          | 144           | 2204                     | 43                      | 6584                    | 0                      |
| /user/register  | 1597            | 64              | 2,4           | 165           | 101          | 144           | 2204                     | 43                      | 6584                    | 0                      |


# Выводы
Разнесение на мастер и слейв не повлияло на la и пропускную способность АПИ, которая выполняла чтение. Однако пропускная способность апи, которая записывала улучшишась на 36,4%.

Кроме того, использование реплики позволило перенести нагрузку с мастера на слейв. Похоже, что запись не создает большой нагрузки на CPU, видимо из-за того, что эта операция тратит много времени на ожидание дискового воода-вывода. 